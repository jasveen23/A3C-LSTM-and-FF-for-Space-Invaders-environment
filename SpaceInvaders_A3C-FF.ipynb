{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INM707 Coursework - Advanced Task\n",
    "# Space Invaders with Asynchronous Advantage Actor-Critic (A3C) - Feedforward\n",
    "### By: Elisabeta Monica Furdui: 190045971 and Jasveen Kaur: 190020638 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required modules\n",
    "For this implementation, we have used OpenAI gym and Pytorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#Code referenced and corrected/edited from: https://github.com/ikostrikov/pytorch-a3c\n",
    "###############\n",
    "\n",
    "from __future__ import print_function\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "from gym import wrappers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "from collections import deque\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "import math\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/openai/universe-starter-agent\n",
    "def create_atari_env(env_id, video=False):\n",
    "    env = gym.make(env_id)\n",
    "    if video:\n",
    "        env = wrappers.Monitor(env, 'test', force=True)\n",
    "    env = MyAtariRescale42x42(env)\n",
    "    env = MyNormalizedEnv(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def _process_frame42(frame):\n",
    "    frame = frame[34:34 + 160, :160]\n",
    "    # Resize by half, then down to 42x42 (essentially mipmapping). If\n",
    "    # we resize directly we lose pixels that, when mapped to 42x42,\n",
    "    # aren't close enough to the pixel boundary.\n",
    "    frame = cv2.resize(frame, (80, 80))\n",
    "    frame = cv2.resize(frame, (42, 42))\n",
    "    frame = frame.mean(2)\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame *= (1.0 / 255.0)\n",
    "    #frame = np.reshape(frame, [1, 42, 42])\n",
    "    return frame\n",
    "\n",
    "\n",
    "class MyAtariRescale42x42(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env=None):\n",
    "        super(MyAtariRescale42x42, self).__init__(env)\n",
    "        self.observation_space = Box(0.0, 1.0, [1, 42, 42], dtype = np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return _process_frame42(observation)\n",
    "\n",
    "\n",
    "class MyNormalizedEnv(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env=None):\n",
    "        super(MyNormalizedEnv, self).__init__(env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.num_steps += 1\n",
    "        self.state_mean = self.state_mean * self.alpha + observation.mean() * (1 - self.alpha)\n",
    "        self.state_std = self.state_std * self.alpha + observation.std() * (1 - self.alpha)\n",
    "\n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        ret = (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "        return np.expand_dims(ret, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedRMSprop(optim.RMSprop):\n",
    "    \"\"\"Implements RMSprop algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False):\n",
    "        super(SharedRMSprop, self).__init__(params, lr, alpha, eps, weight_decay, momentum, centered)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SharedRMSprop, self).__setstate__(state)\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['square_avg'].share_memory_()\n",
    "                state['step'].share_memory_()\n",
    "                state['grad_avg'].share_memory_()\n",
    "                state['momentum_buffer'].share_memory_()\n",
    "\n",
    "    def step(self):\n",
    "        loss = None\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "\n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n",
    "                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                if group['momentum'] > 0:\n",
    "                    buf = state['momentum_buffer']\n",
    "                    buf.mul_(group['momentum']).addcdiv_(grad, avg)\n",
    "                    p.data.add_(-group['lr'], buf)\n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class SharedAdam(optim.Adam):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "    def step(self):\n",
    "        loss = None\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                bias_correction1 = 1 - beta1 ** state['step'].item()\n",
    "                bias_correction2 = 1 - beta2 ** state['step'].item()\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the weights and defining the A3C Feedforward network architecture \n",
    "Since the agents share the same network, therefore they also share common weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing and setting the variance of a tensor of weights\n",
    "def normalized_columns_initializer(weights, std=1.0):\n",
    "    out = torch.randn(weights.size())\n",
    "    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim = True))\n",
    "    return out\n",
    "\n",
    "# Initializing the weights of the neural network in an optimal way for the learning\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "#A3C Feedforward Model\n",
    "class ActorCritic(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.fc = nn.Linear(32 * 3 * 3, 256)\n",
    "        \n",
    "        num_outputs = action_space.n\n",
    "        self.critic_linear = nn.Linear(256, 1)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs)\n",
    "        \n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = normalized_columns_initializer(self.actor_linear.weight.data, 0.01)\n",
    "        self.critic_linear.weight.data = normalized_columns_initializer(self.critic_linear.weight.data, 1.0)\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        inputs = inputs[0]\n",
    "        \n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = x.view(-1, 32 * 3 * 3)\n",
    "        x = F.relu(self.fc(x))\n",
    "        \n",
    "        return self.critic_linear(x), self.actor_linear(x), x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_shared_grads(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        if shared_param.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad\n",
    "\n",
    "def train(rank, params, shared_model, optimizer):\n",
    "    torch.manual_seed(params.seed + rank)\n",
    "    env = create_atari_env(params.env_name)\n",
    "    env.seed(params.seed + rank)\n",
    "    \n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    done = True\n",
    "    \n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        episode_length += 1\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "        if done:\n",
    "            x = Variable(torch.zeros(1, 256))\n",
    "            #hx = Variable(torch.zeros(1, 256))\n",
    "        else:\n",
    "            x = Variable(x.data)\n",
    "            #hx = Variable(hx.data)\n",
    "            \n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "        \n",
    "        for step in range(params.num_steps):\n",
    "            value, action_values, x = model((Variable(state.unsqueeze(0)), x))\n",
    "            prob = F.softmax(action_values, dim = 1)\n",
    "            log_prob = F.log_softmax(action_values, dim = 1)\n",
    "            entropy = -(log_prob * prob).sum(1)\n",
    "            \n",
    "            entropies.append(entropy)\n",
    "            action = prob.multinomial(num_samples = 1).data\n",
    "            log_prob = log_prob.gather(1, Variable(action))\n",
    "            \n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "            done = (done or episode_length >= params.max_episode_length)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            \n",
    "            if done:\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "            \n",
    "            state = torch.from_numpy(state)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            value, _, _ = model((Variable(state.unsqueeze(0)), x))\n",
    "            R = value.data \n",
    "        \n",
    "        values.append(Variable(R))\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        R = Variable(R)\n",
    "        gae = torch.zeros(1, 1)\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = params.gamma * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "            TD = rewards[i] + params.gamma * values[i + 1].data - values[i].data\n",
    "            gae = gae * params.gamma * params.tau + TD\n",
    "            policy_loss = policy_loss - log_probs[i] * Variable(gae) - 0.01 * entropies[i]\n",
    "        optimizer.zero_grad()\n",
    "        (policy_loss + 0.5 * value_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
    "        ensure_shared_grads(model, shared_model)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to test the agent's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory(params):\n",
    "    # logging\n",
    "    log_dir = os.path.join('logfile', params.model_name)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    log_filename = params.env_name+\".\"+ params.model_name+\".log\"\n",
    "    f = open(os.path.join(log_dir, log_filename), \"w\")\n",
    "    # model saver\n",
    "    ckpt_dir = os.path.join('picklefile', params.model_name)\n",
    "    \n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    ckpt_filename = params.env_name +\".\"+ params.model_name +\".pkl\"\n",
    "    return (f, os.path.join(ckpt_dir, ckpt_filename)), (log_dir, ckpt_dir)\n",
    "\n",
    "def test(rank, params, shared_model):\n",
    "    torch.manual_seed(params.seed + rank)\n",
    "    env = create_atari_env(params.env_name, video=True)\n",
    "    env.seed(params.seed + rank)\n",
    "    \n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "    model.eval()\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    \n",
    "    (f, ckpt_path), (log_dir, ckpt_dir) = directory(params)\n",
    "    \n",
    "    reward_sum = 0\n",
    "    done = True\n",
    "    start_time = time.time()\n",
    "    actions = deque(maxlen=100)\n",
    "    episode_length = 0\n",
    "    episode_i = 0\n",
    "    while True:\n",
    "        episode_length += 1\n",
    "        if done:\n",
    "            model.load_state_dict(shared_model.state_dict())\n",
    "            with torch.no_grad():\n",
    "                x = Variable(torch.zeros(1, 256))\n",
    "                #hx = Variable(torch.zeros(1, 256))\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                x = Variable(x.data)\n",
    "                #hx = Variable(hx.data)\n",
    "        with torch.no_grad():\n",
    "            value, action_value, x = model((Variable(state.unsqueeze(0)), x))\n",
    "            \n",
    "        prob = F.softmax(action_value, dim = 1)\n",
    "        action = prob.max(1)[1].data.numpy()\n",
    "        state, reward, done, _ = env.step(action[0])\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done:\n",
    "            episode_i += 1\n",
    "            if episode_i % params.save_freq == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(ckpt_dir, params.env_name + \".\" + params.model_name +\".\" + str(episode_i) + \".pkl\"))\n",
    "        \n",
    "            info = \"Time {}, episode reward {}, episode length {}\".format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start_time)), reward_sum, episode_length)\n",
    "            print(info)\n",
    "            f.write(info + '\\n')\n",
    "            #print(\"Time {}, episode reward {}, episode length {}\".format(time.strftime(\"%Hh %Mm %Ss\", time.gmtime(time.time() - start_time)), reward_sum, episode_length))\n",
    "            reward_sum = 0\n",
    "            episode_length = 0\n",
    "            actions.clear()\n",
    "            state = env.reset()\n",
    "            time.sleep(60)\n",
    "        state = torch.from_numpy(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the parameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.lr = 0.0001\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 1.\n",
    "        self.seed = 1\n",
    "        self.num_processes = mp.cpu_count()\n",
    "        self.num_steps = 20\n",
    "        self.max_episode_length = 10000\n",
    "        self.env_name = 'SpaceInvaders-v0'\n",
    "        self.task = 'eval'\n",
    "        self.save_freq = 20\n",
    "        self.model_name = 'ActorCritic'\n",
    "        self.load_ckpt = 'picklefile/ActorCritic/SpaceInvaders-v0.ActorCritic.5080.pkl'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running all the agents in parallel with 7 processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 00h 00m 10s, episode reward 445.0, episode length 1072\n",
      "Time 00h 01m 18s, episode reward 320.0, episode length 683\n",
      "Time 00h 02m 30s, episode reward 715.0, episode length 1191\n",
      "Time 00h 03m 35s, episode reward 210.0, episode length 506\n",
      "Time 00h 04m 41s, episode reward 365.0, episode length 640\n",
      "Time 00h 05m 49s, episode reward 435.0, episode length 831\n",
      "Time 00h 06m 59s, episode reward 600.0, episode length 1184\n",
      "Time 00h 08m 09s, episode reward 780.0, episode length 1140\n",
      "Time 00h 09m 16s, episode reward 315.0, episode length 714\n",
      "Time 00h 10m 26s, episode reward 515.0, episode length 951\n",
      "Time 00h 11m 33s, episode reward 360.0, episode length 659\n",
      "Time 00h 12m 43s, episode reward 575.0, episode length 1125\n",
      "Time 00h 13m 52s, episode reward 490.0, episode length 935\n",
      "Time 00h 15m 01s, episode reward 490.0, episode length 914\n",
      "Time 00h 16m 11s, episode reward 570.0, episode length 1147\n",
      "Time 00h 17m 23s, episode reward 715.0, episode length 1273\n",
      "Time 00h 18m 32s, episode reward 600.0, episode length 1016\n",
      "Time 00h 19m 41s, episode reward 525.0, episode length 911\n",
      "Time 00h 20m 50s, episode reward 575.0, episode length 941\n",
      "Time 00h 21m 58s, episode reward 540.0, episode length 956\n",
      "Time 00h 23m 07s, episode reward 600.0, episode length 932\n",
      "Time 00h 24m 17s, episode reward 605.0, episode length 1032\n",
      "Time 00h 25m 27s, episode reward 515.0, episode length 968\n",
      "Time 00h 26m 38s, episode reward 575.0, episode length 1166\n",
      "Time 00h 27m 50s, episode reward 600.0, episode length 1159\n",
      "Time 00h 29m 03s, episode reward 745.0, episode length 1279\n",
      "Time 00h 30m 11s, episode reward 520.0, episode length 810\n",
      "Time 00h 31m 22s, episode reward 575.0, episode length 1103\n",
      "Time 00h 32m 34s, episode reward 545.0, episode length 1236\n",
      "Time 00h 33m 48s, episode reward 785.0, episode length 1415\n",
      "Time 00h 35m 00s, episode reward 545.0, episode length 1184\n",
      "Time 00h 36m 06s, episode reward 285.0, episode length 572\n",
      "Time 00h 37m 15s, episode reward 545.0, episode length 942\n",
      "Time 00h 38m 25s, episode reward 550.0, episode length 873\n",
      "Time 00h 39m 35s, episode reward 600.0, episode length 1070\n",
      "Time 00h 40m 45s, episode reward 515.0, episode length 947\n",
      "Time 00h 41m 53s, episode reward 375.0, episode length 749\n",
      "Time 00h 43m 05s, episode reward 555.0, episode length 1257\n",
      "Time 00h 44m 21s, episode reward 880.0, episode length 1586\n",
      "Time 00h 45m 31s, episode reward 610.0, episode length 1033\n",
      "Time 00h 46m 39s, episode reward 360.0, episode length 647\n",
      "Time 00h 47m 47s, episode reward 520.0, episode length 826\n",
      "Time 00h 48m 54s, episode reward 485.0, episode length 723\n",
      "Time 00h 50m 05s, episode reward 600.0, episode length 1147\n",
      "Time 00h 51m 13s, episode reward 415.0, episode length 721\n",
      "Time 00h 52m 22s, episode reward 430.0, episode length 912\n",
      "Time 00h 53m 31s, episode reward 460.0, episode length 846\n",
      "Time 00h 54m 41s, episode reward 740.0, episode length 978\n",
      "Time 00h 55m 49s, episode reward 510.0, episode length 815\n",
      "Time 00h 56m 59s, episode reward 460.0, episode length 1148\n",
      "Time 00h 58m 07s, episode reward 420.0, episode length 737\n",
      "Time 00h 59m 18s, episode reward 575.0, episode length 1100\n",
      "Time 01h 00m 27s, episode reward 575.0, episode length 965\n",
      "Time 01h 01m 37s, episode reward 545.0, episode length 1031\n",
      "Time 01h 02m 47s, episode reward 515.0, episode length 1005\n",
      "Time 01h 04m 00s, episode reward 700.0, episode length 1379\n",
      "Time 01h 05m 11s, episode reward 570.0, episode length 1086\n",
      "Time 01h 06m 19s, episode reward 630.0, episode length 808\n",
      "Time 01h 07m 25s, episode reward 285.0, episode length 574\n",
      "Time 01h 08m 35s, episode reward 715.0, episode length 1014\n",
      "Time 01h 09m 46s, episode reward 545.0, episode length 1179\n",
      "Time 01h 10m 56s, episode reward 515.0, episode length 965\n",
      "Time 01h 12m 06s, episode reward 545.0, episode length 959\n",
      "Time 01h 13m 17s, episode reward 570.0, episode length 1094\n",
      "Time 01h 14m 28s, episode reward 545.0, episode length 1173\n",
      "Time 01h 15m 38s, episode reward 545.0, episode length 1041\n",
      "Time 01h 16m 49s, episode reward 520.0, episode length 1069\n",
      "Time 01h 17m 57s, episode reward 360.0, episode length 722\n",
      "Time 01h 19m 09s, episode reward 715.0, episode length 1157\n",
      "Time 01h 20m 20s, episode reward 610.0, episode length 981\n",
      "Time 01h 21m 29s, episode reward 555.0, episode length 1051\n",
      "Time 01h 22m 37s, episode reward 465.0, episode length 818\n",
      "Time 01h 23m 46s, episode reward 470.0, episode length 798\n",
      "Time 01h 24m 55s, episode reward 405.0, episode length 789\n",
      "Time 01h 26m 02s, episode reward 320.0, episode length 704\n",
      "Time 01h 27m 12s, episode reward 510.0, episode length 934\n",
      "Time 01h 28m 25s, episode reward 800.0, episode length 1352\n",
      "Time 01h 29m 33s, episode reward 425.0, episode length 757\n",
      "Time 01h 30m 42s, episode reward 515.0, episode length 852\n",
      "Time 01h 31m 51s, episode reward 510.0, episode length 866\n",
      "Time 01h 33m 03s, episode reward 600.0, episode length 1306\n",
      "Time 01h 34m 11s, episode reward 485.0, episode length 796\n",
      "Time 01h 35m 21s, episode reward 485.0, episode length 945\n",
      "Time 01h 36m 30s, episode reward 545.0, episode length 1055\n",
      "Time 01h 37m 40s, episode reward 550.0, episode length 971\n",
      "Time 01h 38m 51s, episode reward 545.0, episode length 1222\n",
      "Time 01h 40m 02s, episode reward 575.0, episode length 1038\n",
      "Time 01h 41m 05s, episode reward 515.0, episode length 841\n",
      "Time 01h 42m 08s, episode reward 715.0, episode length 1303\n",
      "Time 01h 43m 11s, episode reward 575.0, episode length 1114\n",
      "Time 01h 44m 13s, episode reward 545.0, episode length 1028\n",
      "Time 01h 45m 16s, episode reward 540.0, episode length 935\n",
      "Time 01h 46m 17s, episode reward 255.0, episode length 487\n",
      "Time 01h 47m 20s, episode reward 600.0, episode length 1011\n",
      "Time 01h 48m 23s, episode reward 715.0, episode length 1160\n",
      "Time 01h 49m 25s, episode reward 380.0, episode length 726\n",
      "Time 01h 50m 28s, episode reward 575.0, episode length 1070\n",
      "Time 01h 51m 32s, episode reward 600.0, episode length 944\n",
      "Time 01h 52m 35s, episode reward 600.0, episode length 1118\n",
      "Time 01h 53m 38s, episode reward 415.0, episode length 743\n",
      "Time 01h 54m 41s, episode reward 820.0, episode length 1374\n",
      "Time 01h 55m 48s, episode reward 485.0, episode length 855\n",
      "Time 01h 56m 52s, episode reward 600.0, episode length 1158\n",
      "Time 01h 57m 54s, episode reward 605.0, episode length 838\n",
      "Time 01h 58m 58s, episode reward 540.0, episode length 992\n",
      "Time 02h 00m 10s, episode reward 725.0, episode length 994\n",
      "Time 02h 01m 20s, episode reward 715.0, episode length 759\n",
      "Time 02h 02m 27s, episode reward 510.0, episode length 922\n",
      "Time 02h 03m 34s, episode reward 600.0, episode length 1077\n",
      "Time 02h 04m 45s, episode reward 460.0, episode length 1125\n",
      "Time 02h 05m 52s, episode reward 600.0, episode length 1180\n",
      "Time 02h 06m 58s, episode reward 435.0, episode length 849\n",
      "Time 02h 08m 02s, episode reward 515.0, episode length 926\n",
      "Time 02h 09m 04s, episode reward 670.0, episode length 899\n",
      "Time 02h 10m 09s, episode reward 700.0, episode length 1496\n",
      "Time 02h 11m 14s, episode reward 980.0, episode length 1823\n",
      "Time 02h 12m 18s, episode reward 570.0, episode length 1096\n",
      "Time 02h 13m 20s, episode reward 510.0, episode length 1026\n",
      "Time 02h 14m 22s, episode reward 375.0, episode length 817\n",
      "Time 02h 15m 25s, episode reward 545.0, episode length 1021\n",
      "Time 02h 16m 27s, episode reward 300.0, episode length 617\n",
      "Time 02h 17m 30s, episode reward 650.0, episode length 1047\n",
      "Time 02h 18m 33s, episode reward 770.0, episode length 1004\n",
      "Time 02h 19m 34s, episode reward 320.0, episode length 656\n",
      "Time 02h 20m 37s, episode reward 485.0, episode length 1072\n",
      "Time 02h 21m 41s, episode reward 305.0, episode length 739\n",
      "Time 02h 22m 44s, episode reward 800.0, episode length 1128\n",
      "Time 02h 23m 49s, episode reward 890.0, episode length 1662\n",
      "Time 02h 24m 52s, episode reward 605.0, episode length 1033\n",
      "Time 02h 25m 55s, episode reward 775.0, episode length 1010\n",
      "Time 02h 26m 58s, episode reward 710.0, episode length 1036\n",
      "Time 02h 28m 01s, episode reward 600.0, episode length 951\n",
      "Time 02h 29m 04s, episode reward 465.0, episode length 953\n",
      "Time 02h 30m 07s, episode reward 605.0, episode length 1112\n",
      "Time 02h 31m 10s, episode reward 540.0, episode length 1086\n",
      "Time 02h 32m 14s, episode reward 765.0, episode length 1444\n",
      "Time 02h 33m 17s, episode reward 575.0, episode length 1087\n",
      "Time 02h 34m 19s, episode reward 515.0, episode length 960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 02h 35m 22s, episode reward 485.0, episode length 846\n",
      "Time 02h 36m 25s, episode reward 570.0, episode length 921\n",
      "Time 02h 37m 27s, episode reward 435.0, episode length 823\n",
      "Time 02h 38m 30s, episode reward 515.0, episode length 854\n",
      "Time 02h 39m 32s, episode reward 600.0, episode length 967\n",
      "Time 02h 40m 35s, episode reward 470.0, episode length 973\n",
      "Time 02h 41m 40s, episode reward 945.0, episode length 1650\n",
      "Time 02h 42m 43s, episode reward 540.0, episode length 995\n",
      "Time 02h 43m 45s, episode reward 520.0, episode length 926\n",
      "Time 02h 44m 50s, episode reward 720.0, episode length 1537\n",
      "Time 02h 45m 52s, episode reward 575.0, episode length 1032\n",
      "Time 02h 46m 56s, episode reward 570.0, episode length 1237\n",
      "Time 02h 47m 58s, episode reward 415.0, episode length 729\n",
      "Time 02h 49m 01s, episode reward 485.0, episode length 896\n",
      "Time 02h 50m 04s, episode reward 465.0, episode length 1066\n",
      "Time 02h 51m 06s, episode reward 555.0, episode length 965\n",
      "Time 02h 52m 09s, episode reward 605.0, episode length 1092\n",
      "Time 02h 53m 13s, episode reward 730.0, episode length 1277\n",
      "Time 02h 54m 16s, episode reward 515.0, episode length 1278\n",
      "Time 02h 55m 19s, episode reward 545.0, episode length 1106\n",
      "Time 02h 56m 23s, episode reward 695.0, episode length 1187\n",
      "Time 02h 57m 27s, episode reward 485.0, episode length 845\n",
      "Time 02h 58m 30s, episode reward 600.0, episode length 1065\n",
      "Time 02h 59m 34s, episode reward 720.0, episode length 1321\n",
      "Time 03h 00m 36s, episode reward 310.0, episode length 649\n",
      "Time 03h 01m 39s, episode reward 465.0, episode length 1100\n",
      "Time 03h 02m 43s, episode reward 680.0, episode length 1301\n",
      "Time 03h 03m 45s, episode reward 440.0, episode length 719\n",
      "Time 03h 04m 47s, episode reward 435.0, episode length 730\n",
      "Time 03h 05m 49s, episode reward 545.0, episode length 1036\n",
      "Time 03h 06m 51s, episode reward 310.0, episode length 623\n",
      "Time 03h 07m 53s, episode reward 320.0, episode length 609\n",
      "Time 03h 08m 56s, episode reward 575.0, episode length 932\n",
      "Time 03h 09m 58s, episode reward 455.0, episode length 848\n",
      "Time 03h 11m 00s, episode reward 445.0, episode length 786\n",
      "Time 03h 12m 04s, episode reward 405.0, episode length 787\n",
      "Time 03h 13m 06s, episode reward 515.0, episode length 990\n",
      "Time 03h 14m 09s, episode reward 600.0, episode length 1100\n",
      "Time 03h 15m 12s, episode reward 540.0, episode length 942\n",
      "Time 03h 16m 15s, episode reward 575.0, episode length 1073\n",
      "Time 03h 17m 17s, episode reward 570.0, episode length 1052\n",
      "Time 03h 18m 20s, episode reward 485.0, episode length 921\n",
      "Time 03h 19m 23s, episode reward 570.0, episode length 1067\n",
      "Time 03h 20m 26s, episode reward 540.0, episode length 1039\n",
      "Time 03h 21m 28s, episode reward 295.0, episode length 566\n",
      "Time 03h 22m 30s, episode reward 545.0, episode length 942\n",
      "Time 03h 23m 33s, episode reward 515.0, episode length 937\n",
      "Time 03h 24m 35s, episode reward 490.0, episode length 896\n",
      "Time 03h 25m 37s, episode reward 325.0, episode length 620\n",
      "Time 03h 26m 39s, episode reward 570.0, episode length 903\n",
      "Time 03h 27m 42s, episode reward 545.0, episode length 1091\n",
      "Time 03h 28m 45s, episode reward 485.0, episode length 1099\n",
      "Time 03h 29m 47s, episode reward 600.0, episode length 972\n",
      "Time 03h 30m 50s, episode reward 575.0, episode length 1115\n",
      "Time 03h 31m 54s, episode reward 760.0, episode length 1409\n",
      "Time 03h 32m 57s, episode reward 460.0, episode length 835\n",
      "Time 03h 33m 59s, episode reward 660.0, episode length 869\n",
      "Time 03h 35m 03s, episode reward 575.0, episode length 1167\n",
      "Time 03h 36m 05s, episode reward 410.0, episode length 754\n",
      "Time 03h 37m 08s, episode reward 430.0, episode length 871\n",
      "Time 03h 38m 10s, episode reward 495.0, episode length 1046\n",
      "Time 03h 39m 12s, episode reward 410.0, episode length 796\n",
      "Time 03h 40m 15s, episode reward 540.0, episode length 995\n",
      "Time 03h 41m 17s, episode reward 545.0, episode length 752\n",
      "Time 03h 42m 20s, episode reward 405.0, episode length 762\n",
      "Time 03h 43m 23s, episode reward 800.0, episode length 1399\n",
      "Time 03h 44m 26s, episode reward 525.0, episode length 1101\n",
      "Time 03h 45m 28s, episode reward 345.0, episode length 627\n",
      "Time 03h 46m 30s, episode reward 340.0, episode length 582\n",
      "Time 03h 47m 33s, episode reward 430.0, episode length 827\n",
      "Time 03h 48m 36s, episode reward 490.0, episode length 1103\n",
      "Time 03h 49m 38s, episode reward 465.0, episode length 794\n",
      "Time 03h 50m 41s, episode reward 545.0, episode length 1050\n",
      "Time 03h 51m 44s, episode reward 525.0, episode length 1015\n",
      "Time 03h 52m 45s, episode reward 315.0, episode length 618\n",
      "Time 03h 53m 48s, episode reward 310.0, episode length 861\n",
      "Time 03h 54m 51s, episode reward 600.0, episode length 1089\n",
      "Time 03h 55m 54s, episode reward 545.0, episode length 1165\n",
      "Time 03h 56m 58s, episode reward 715.0, episode length 1140\n",
      "Time 03h 58m 00s, episode reward 295.0, episode length 579\n",
      "Time 03h 59m 03s, episode reward 455.0, episode length 940\n",
      "Time 04h 00m 06s, episode reward 605.0, episode length 1114\n",
      "Time 04h 01m 09s, episode reward 390.0, episode length 769\n",
      "Time 04h 02m 14s, episode reward 960.0, episode length 1618\n",
      "Time 04h 03m 18s, episode reward 550.0, episode length 1124\n",
      "Time 04h 04m 21s, episode reward 545.0, episode length 961\n",
      "Time 04h 05m 23s, episode reward 345.0, episode length 759\n",
      "Time 04h 06m 25s, episode reward 545.0, episode length 943\n",
      "Time 04h 07m 28s, episode reward 600.0, episode length 1049\n",
      "Time 04h 08m 30s, episode reward 500.0, episode length 819\n",
      "Time 04h 09m 32s, episode reward 575.0, episode length 1028\n",
      "Time 04h 10m 34s, episode reward 570.0, episode length 881\n",
      "Time 04h 11m 37s, episode reward 490.0, episode length 864\n",
      "Time 04h 12m 49s, episode reward 745.0, episode length 1179\n",
      "Time 04h 13m 58s, episode reward 575.0, episode length 1088\n",
      "Time 04h 15m 09s, episode reward 630.0, episode length 1147\n",
      "Time 04h 16m 15s, episode reward 440.0, episode length 701\n",
      "Time 04h 17m 25s, episode reward 570.0, episode length 1023\n",
      "Time 04h 18m 34s, episode reward 505.0, episode length 863\n",
      "Time 04h 19m 44s, episode reward 485.0, episode length 1004\n",
      "Time 04h 20m 54s, episode reward 420.0, episode length 865\n",
      "Time 04h 22m 06s, episode reward 600.0, episode length 1188\n",
      "Time 04h 23m 16s, episode reward 490.0, episode length 922\n",
      "Time 04h 24m 27s, episode reward 485.0, episode length 1031\n",
      "Time 04h 25m 38s, episode reward 805.0, episode length 1089\n",
      "Time 04h 26m 47s, episode reward 460.0, episode length 877\n",
      "Time 04h 27m 56s, episode reward 515.0, episode length 845\n",
      "Time 04h 29m 04s, episode reward 295.0, episode length 688\n",
      "Time 04h 30m 14s, episode reward 425.0, episode length 955\n",
      "Time 04h 31m 22s, episode reward 450.0, episode length 769\n",
      "Time 04h 32m 34s, episode reward 600.0, episode length 1190\n",
      "Time 04h 33m 42s, episode reward 330.0, episode length 708\n",
      "Time 04h 34m 57s, episode reward 730.0, episode length 1391\n",
      "Time 04h 36m 05s, episode reward 365.0, episode length 776\n",
      "Time 04h 37m 15s, episode reward 545.0, episode length 998\n",
      "Time 04h 38m 25s, episode reward 460.0, episode length 836\n",
      "Time 04h 39m 36s, episode reward 570.0, episode length 1097\n",
      "Time 04h 40m 45s, episode reward 460.0, episode length 944\n",
      "Time 04h 41m 55s, episode reward 515.0, episode length 877\n",
      "Time 04h 43m 06s, episode reward 600.0, episode length 1063\n",
      "Time 04h 44m 16s, episode reward 740.0, episode length 1036\n",
      "Time 04h 45m 27s, episode reward 480.0, episode length 1130\n",
      "Time 04h 46m 40s, episode reward 600.0, episode length 1291\n",
      "Time 04h 47m 52s, episode reward 510.0, episode length 1156\n",
      "Time 04h 49m 04s, episode reward 680.0, episode length 1132\n",
      "Time 04h 50m 13s, episode reward 385.0, episode length 747\n",
      "Time 04h 51m 23s, episode reward 530.0, episode length 1025\n",
      "Time 04h 52m 37s, episode reward 770.0, episode length 1440\n",
      "Time 04h 53m 46s, episode reward 490.0, episode length 942\n",
      "Time 04h 55m 03s, episode reward 910.0, episode length 1690\n",
      "Time 04h 56m 14s, episode reward 575.0, episode length 1092\n",
      "Time 04h 57m 26s, episode reward 520.0, episode length 1052\n",
      "Time 04h 58m 32s, episode reward 350.0, episode length 659\n",
      "Time 04h 59m 47s, episode reward 715.0, episode length 1442\n",
      "Time 05h 01m 00s, episode reward 570.0, episode length 1313\n",
      "Time 05h 02m 13s, episode reward 800.0, episode length 1136\n",
      "Time 05h 03m 19s, episode reward 225.0, episode length 614\n",
      "Time 05h 04m 31s, episode reward 600.0, episode length 1166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 05h 05m 41s, episode reward 515.0, episode length 863\n",
      "Time 05h 06m 48s, episode reward 405.0, episode length 745\n",
      "Time 05h 07m 58s, episode reward 460.0, episode length 890\n",
      "Time 05h 09m 11s, episode reward 600.0, episode length 1263\n",
      "Time 05h 10m 21s, episode reward 600.0, episode length 1068\n",
      "Time 05h 11m 33s, episode reward 570.0, episode length 1172\n",
      "Time 05h 12m 48s, episode reward 695.0, episode length 1439\n",
      "Time 05h 13m 57s, episode reward 360.0, episode length 754\n",
      "Time 05h 15m 08s, episode reward 570.0, episode length 1132\n",
      "Time 05h 16m 18s, episode reward 465.0, episode length 867\n",
      "Time 05h 17m 29s, episode reward 600.0, episode length 1100\n",
      "Time 05h 18m 41s, episode reward 545.0, episode length 1127\n",
      "Time 05h 19m 50s, episode reward 405.0, episode length 751\n",
      "Time 05h 20m 57s, episode reward 345.0, episode length 691\n",
      "Time 05h 22m 04s, episode reward 305.0, episode length 641\n",
      "Time 05h 23m 11s, episode reward 280.0, episode length 617\n",
      "Time 05h 24m 24s, episode reward 545.0, episode length 1151\n",
      "Time 05h 25m 33s, episode reward 485.0, episode length 932\n",
      "Time 05h 26m 45s, episode reward 555.0, episode length 1122\n",
      "Time 05h 27m 54s, episode reward 485.0, episode length 859\n",
      "Time 05h 29m 05s, episode reward 545.0, episode length 1063\n",
      "Time 05h 30m 13s, episode reward 295.0, episode length 655\n",
      "Time 05h 31m 26s, episode reward 520.0, episode length 1300\n",
      "Time 05h 32m 35s, episode reward 365.0, episode length 745\n",
      "Time 05h 33m 44s, episode reward 485.0, episode length 839\n",
      "Time 05h 34m 51s, episode reward 435.0, episode length 780\n",
      "Time 05h 36m 03s, episode reward 720.0, episode length 1182\n",
      "Time 05h 37m 11s, episode reward 460.0, episode length 820\n",
      "Time 05h 38m 19s, episode reward 420.0, episode length 729\n",
      "Time 05h 39m 31s, episode reward 570.0, episode length 1218\n",
      "Time 05h 40m 42s, episode reward 495.0, episode length 974\n",
      "Time 05h 41m 51s, episode reward 425.0, episode length 846\n",
      "Time 05h 43m 04s, episode reward 555.0, episode length 1217\n",
      "Time 05h 44m 16s, episode reward 520.0, episode length 1042\n",
      "Time 05h 45m 25s, episode reward 545.0, episode length 1076\n",
      "Time 11h 03m 42s, episode reward 460.0, episode length 922\n",
      "Time 11h 04m 44s, episode reward 385.0, episode length 677\n",
      "Time 11h 05m 47s, episode reward 490.0, episode length 1038\n",
      "Time 11h 06m 49s, episode reward 485.0, episode length 833\n",
      "Time 11h 07m 52s, episode reward 665.0, episode length 1162\n",
      "Time 11h 08m 54s, episode reward 475.0, episode length 832\n",
      "Time 11h 09m 56s, episode reward 575.0, episode length 837\n",
      "Time 11h 10m 59s, episode reward 545.0, episode length 1035\n",
      "Time 11h 12m 01s, episode reward 470.0, episode length 923\n",
      "Time 11h 13m 04s, episode reward 520.0, episode length 974\n",
      "Time 11h 14m 06s, episode reward 380.0, episode length 779\n",
      "Time 11h 15m 10s, episode reward 575.0, episode length 1120\n",
      "Time 11h 16m 12s, episode reward 545.0, episode length 954\n",
      "Time 11h 17m 14s, episode reward 540.0, episode length 1026\n",
      "Time 11h 18m 17s, episode reward 570.0, episode length 1034\n",
      "Time 11h 19m 20s, episode reward 485.0, episode length 1031\n",
      "Time 11h 20m 22s, episode reward 545.0, episode length 1051\n",
      "Time 11h 21m 25s, episode reward 600.0, episode length 1104\n",
      "Time 11h 22m 27s, episode reward 465.0, episode length 783\n",
      "Time 11h 23m 30s, episode reward 655.0, episode length 1221\n",
      "Time 11h 24m 33s, episode reward 600.0, episode length 1184\n",
      "Time 11h 25m 35s, episode reward 550.0, episode length 1071\n",
      "Time 11h 26m 38s, episode reward 510.0, episode length 901\n",
      "Time 11h 27m 39s, episode reward 305.0, episode length 687\n",
      "Time 11h 28m 43s, episode reward 715.0, episode length 1308\n",
      "Time 11h 29m 45s, episode reward 570.0, episode length 1126\n",
      "Time 11h 30m 48s, episode reward 455.0, episode length 816\n",
      "Time 11h 31m 50s, episode reward 530.0, episode length 1122\n",
      "Time 11h 32m 53s, episode reward 430.0, episode length 847\n",
      "Time 11h 33m 55s, episode reward 685.0, episode length 1017\n",
      "Time 11h 34m 57s, episode reward 410.0, episode length 695\n",
      "Time 11h 35m 59s, episode reward 570.0, episode length 956\n",
      "Time 11h 37m 04s, episode reward 550.0, episode length 1132\n",
      "Time 11h 38m 07s, episode reward 465.0, episode length 933\n",
      "Time 11h 39m 09s, episode reward 545.0, episode length 941\n",
      "Time 11h 40m 12s, episode reward 575.0, episode length 1153\n",
      "Time 11h 41m 15s, episode reward 490.0, episode length 933\n",
      "Time 11h 42m 18s, episode reward 545.0, episode length 1076\n",
      "Time 11h 43m 21s, episode reward 570.0, episode length 946\n",
      "Time 11h 44m 24s, episode reward 525.0, episode length 1106\n",
      "Time 11h 45m 27s, episode reward 510.0, episode length 1057\n",
      "Time 11h 46m 30s, episode reward 545.0, episode length 1081\n",
      "Time 11h 47m 33s, episode reward 570.0, episode length 1059\n",
      "Time 11h 48m 37s, episode reward 610.0, episode length 791\n",
      "Time 11h 49m 40s, episode reward 455.0, episode length 892\n",
      "Time 11h 50m 45s, episode reward 880.0, episode length 1847\n",
      "Time 11h 51m 48s, episode reward 575.0, episode length 1068\n",
      "Time 11h 52m 50s, episode reward 425.0, episode length 828\n",
      "Time 11h 53m 54s, episode reward 710.0, episode length 1340\n",
      "Time 11h 54m 56s, episode reward 535.0, episode length 911\n",
      "Time 11h 55m 58s, episode reward 495.0, episode length 833\n",
      "Time 11h 57m 01s, episode reward 545.0, episode length 1099\n",
      "Time 11h 58m 04s, episode reward 570.0, episode length 1137\n",
      "Time 11h 59m 07s, episode reward 570.0, episode length 1085\n",
      "Time 12h 00m 10s, episode reward 600.0, episode length 1110\n",
      "Time 12h 01m 13s, episode reward 465.0, episode length 940\n",
      "Time 12h 02m 15s, episode reward 265.0, episode length 561\n",
      "Time 12h 03m 18s, episode reward 695.0, episode length 1045\n",
      "Time 12h 04m 21s, episode reward 575.0, episode length 834\n",
      "Time 12h 05m 23s, episode reward 515.0, episode length 872\n",
      "Time 12h 06m 25s, episode reward 430.0, episode length 724\n",
      "Time 12h 07m 27s, episode reward 445.0, episode length 764\n",
      "Time 12h 08m 30s, episode reward 715.0, episode length 1173\n",
      "Time 12h 09m 34s, episode reward 545.0, episode length 1181\n",
      "Time 12h 10m 37s, episode reward 600.0, episode length 1240\n",
      "Time 12h 11m 39s, episode reward 450.0, episode length 800\n",
      "Time 12h 12m 43s, episode reward 630.0, episode length 1172\n",
      "Time 12h 13m 45s, episode reward 315.0, episode length 729\n",
      "Time 12h 14m 48s, episode reward 545.0, episode length 1024\n",
      "Time 12h 15m 50s, episode reward 440.0, episode length 770\n",
      "Time 12h 16m 52s, episode reward 660.0, episode length 772\n",
      "Time 12h 17m 56s, episode reward 600.0, episode length 1152\n",
      "Time 12h 19m 00s, episode reward 600.0, episode length 1179\n",
      "Time 12h 20m 05s, episode reward 885.0, episode length 1576\n",
      "Time 12h 21m 07s, episode reward 465.0, episode length 890\n",
      "Time 12h 22m 11s, episode reward 510.0, episode length 1198\n",
      "Time 12h 23m 13s, episode reward 380.0, episode length 724\n",
      "Time 12h 24m 17s, episode reward 605.0, episode length 1152\n",
      "Time 12h 25m 19s, episode reward 385.0, episode length 696\n",
      "Time 12h 26m 22s, episode reward 515.0, episode length 982\n",
      "Time 12h 27m 26s, episode reward 545.0, episode length 1277\n",
      "Time 12h 28m 29s, episode reward 520.0, episode length 829\n",
      "Time 12h 29m 32s, episode reward 570.0, episode length 1166\n",
      "Time 12h 30m 35s, episode reward 570.0, episode length 1102\n",
      "Time 12h 31m 37s, episode reward 440.0, episode length 807\n",
      "Time 12h 32m 40s, episode reward 485.0, episode length 829\n",
      "Time 12h 33m 44s, episode reward 715.0, episode length 1229\n",
      "Time 12h 34m 47s, episode reward 575.0, episode length 1009\n",
      "Time 12h 35m 49s, episode reward 445.0, episode length 784\n",
      "Time 12h 36m 52s, episode reward 495.0, episode length 1129\n",
      "Time 12h 37m 54s, episode reward 265.0, episode length 624\n",
      "Time 12h 38m 56s, episode reward 600.0, episode length 964\n",
      "Time 12h 39m 59s, episode reward 545.0, episode length 1109\n",
      "Time 12h 41m 02s, episode reward 485.0, episode length 896\n",
      "Time 12h 42m 05s, episode reward 600.0, episode length 1167\n",
      "Time 12h 43m 08s, episode reward 570.0, episode length 956\n",
      "Time 12h 44m 10s, episode reward 395.0, episode length 689\n",
      "Time 12h 45m 13s, episode reward 490.0, episode length 731\n",
      "Time 12h 46m 17s, episode reward 600.0, episode length 1135\n",
      "Time 12h 47m 20s, episode reward 520.0, episode length 1040\n",
      "Time 12h 48m 21s, episode reward 485.0, episode length 807\n",
      "Time 12h 49m 24s, episode reward 690.0, episode length 1157\n",
      "Time 12h 50m 27s, episode reward 580.0, episode length 1178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 12h 51m 30s, episode reward 430.0, episode length 849\n",
      "Time 12h 52m 33s, episode reward 605.0, episode length 1030\n",
      "Time 12h 53m 35s, episode reward 575.0, episode length 1000\n",
      "Time 12h 54m 38s, episode reward 550.0, episode length 947\n",
      "Time 12h 55m 41s, episode reward 570.0, episode length 1045\n",
      "Time 12h 56m 43s, episode reward 605.0, episode length 1068\n",
      "Time 12h 57m 46s, episode reward 540.0, episode length 915\n",
      "Time 12h 58m 49s, episode reward 570.0, episode length 967\n",
      "Time 12h 59m 54s, episode reward 760.0, episode length 1517\n",
      "Time 13h 00m 57s, episode reward 640.0, episode length 1179\n",
      "Time 13h 02m 00s, episode reward 515.0, episode length 1041\n",
      "Time 13h 03m 02s, episode reward 355.0, episode length 773\n",
      "Time 13h 04m 05s, episode reward 540.0, episode length 1093\n",
      "Time 13h 05m 08s, episode reward 680.0, episode length 1196\n",
      "Time 13h 06m 10s, episode reward 455.0, episode length 838\n",
      "Time 13h 07m 13s, episode reward 435.0, episode length 840\n",
      "Time 13h 08m 15s, episode reward 295.0, episode length 641\n",
      "Time 13h 09m 18s, episode reward 600.0, episode length 1274\n",
      "Time 13h 10m 20s, episode reward 390.0, episode length 799\n",
      "Time 13h 11m 23s, episode reward 570.0, episode length 1183\n",
      "Time 13h 12m 26s, episode reward 610.0, episode length 995\n",
      "Time 13h 13m 29s, episode reward 485.0, episode length 940\n",
      "Time 13h 14m 31s, episode reward 415.0, episode length 801\n",
      "Time 13h 15m 34s, episode reward 525.0, episode length 930\n",
      "Time 13h 16m 38s, episode reward 570.0, episode length 1282\n",
      "Time 13h 17m 41s, episode reward 550.0, episode length 1027\n",
      "Time 13h 18m 44s, episode reward 515.0, episode length 1159\n",
      "Time 13h 19m 47s, episode reward 545.0, episode length 1079\n",
      "Time 13h 20m 51s, episode reward 550.0, episode length 1100\n",
      "Time 13h 21m 53s, episode reward 525.0, episode length 979\n",
      "Time 13h 22m 57s, episode reward 740.0, episode length 1381\n",
      "Time 13h 23m 59s, episode reward 430.0, episode length 802\n",
      "Time 13h 25m 01s, episode reward 385.0, episode length 666\n",
      "Time 13h 26m 04s, episode reward 840.0, episode length 1481\n",
      "Time 13h 27m 07s, episode reward 510.0, episode length 836\n",
      "Time 13h 28m 09s, episode reward 415.0, episode length 740\n",
      "Time 13h 29m 12s, episode reward 465.0, episode length 801\n",
      "Time 13h 30m 15s, episode reward 455.0, episode length 937\n",
      "Time 13h 31m 18s, episode reward 600.0, episode length 1089\n",
      "Time 13h 32m 21s, episode reward 720.0, episode length 992\n",
      "Time 13h 33m 24s, episode reward 480.0, episode length 1049\n",
      "Time 13h 34m 26s, episode reward 295.0, episode length 553\n",
      "Time 13h 35m 29s, episode reward 815.0, episode length 1124\n",
      "Time 13h 36m 31s, episode reward 545.0, episode length 821\n",
      "Time 13h 37m 34s, episode reward 525.0, episode length 909\n",
      "Time 13h 38m 36s, episode reward 440.0, episode length 861\n",
      "Time 13h 39m 38s, episode reward 415.0, episode length 740\n",
      "Time 13h 40m 40s, episode reward 295.0, episode length 704\n",
      "Time 13h 41m 44s, episode reward 900.0, episode length 1502\n",
      "Time 13h 42m 47s, episode reward 550.0, episode length 974\n",
      "Time 13h 43m 49s, episode reward 430.0, episode length 851\n",
      "Time 13h 44m 51s, episode reward 545.0, episode length 914\n",
      "Time 13h 45m 54s, episode reward 600.0, episode length 1046\n",
      "Time 13h 46m 56s, episode reward 375.0, episode length 679\n",
      "Time 13h 47m 59s, episode reward 740.0, episode length 1290\n",
      "Time 13h 49m 01s, episode reward 465.0, episode length 746\n",
      "Time 13h 50m 04s, episode reward 545.0, episode length 1204\n",
      "Time 13h 51m 07s, episode reward 515.0, episode length 977\n",
      "Time 13h 52m 10s, episode reward 575.0, episode length 1114\n",
      "Time 13h 53m 13s, episode reward 550.0, episode length 1107\n",
      "Time 13h 54m 16s, episode reward 490.0, episode length 833\n",
      "Time 13h 55m 19s, episode reward 460.0, episode length 878\n",
      "Time 13h 56m 20s, episode reward 515.0, episode length 788\n",
      "Time 13h 57m 24s, episode reward 540.0, episode length 1117\n",
      "Time 13h 58m 26s, episode reward 715.0, episode length 900\n",
      "Time 13h 59m 29s, episode reward 575.0, episode length 1143\n",
      "Time 14h 00m 32s, episode reward 605.0, episode length 1205\n",
      "Time 14h 01m 35s, episode reward 525.0, episode length 868\n",
      "Time 14h 02m 37s, episode reward 485.0, episode length 881\n",
      "Time 14h 03m 39s, episode reward 460.0, episode length 809\n",
      "Time 14h 04m 42s, episode reward 570.0, episode length 908\n",
      "Time 14h 05m 46s, episode reward 720.0, episode length 1161\n",
      "Time 14h 06m 47s, episode reward 260.0, episode length 614\n",
      "Time 14h 07m 51s, episode reward 800.0, episode length 1115\n",
      "Time 14h 08m 53s, episode reward 400.0, episode length 801\n",
      "Time 14h 09m 56s, episode reward 760.0, episode length 899\n",
      "Time 14h 10m 59s, episode reward 700.0, episode length 1222\n",
      "Time 14h 12m 02s, episode reward 600.0, episode length 1201\n",
      "Time 14h 13m 05s, episode reward 540.0, episode length 983\n",
      "Time 14h 14m 08s, episode reward 550.0, episode length 1172\n",
      "Time 14h 15m 10s, episode reward 490.0, episode length 847\n",
      "Time 14h 16m 13s, episode reward 715.0, episode length 1207\n",
      "Time 14h 17m 17s, episode reward 600.0, episode length 1057\n",
      "Time 14h 18m 19s, episode reward 540.0, episode length 988\n",
      "Time 14h 19m 22s, episode reward 490.0, episode length 942\n",
      "Time 14h 20m 25s, episode reward 540.0, episode length 1203\n",
      "Time 14h 21m 27s, episode reward 330.0, episode length 623\n",
      "Time 14h 22m 29s, episode reward 235.0, episode length 580\n",
      "Time 14h 23m 33s, episode reward 735.0, episode length 1380\n",
      "Time 14h 24m 36s, episode reward 495.0, episode length 998\n",
      "Time 14h 25m 38s, episode reward 350.0, episode length 782\n",
      "Time 14h 26m 41s, episode reward 465.0, episode length 832\n",
      "Time 14h 27m 43s, episode reward 515.0, episode length 797\n",
      "Time 14h 28m 45s, episode reward 570.0, episode length 997\n",
      "Time 14h 29m 48s, episode reward 575.0, episode length 1047\n",
      "Time 14h 30m 50s, episode reward 335.0, episode length 661\n",
      "Time 14h 31m 54s, episode reward 600.0, episode length 1203\n",
      "Time 14h 32m 57s, episode reward 510.0, episode length 896\n",
      "Time 14h 34m 02s, episode reward 630.0, episode length 1090\n",
      "Time 14h 35m 04s, episode reward 455.0, episode length 894\n",
      "Time 14h 36m 07s, episode reward 605.0, episode length 942\n",
      "Time 14h 37m 09s, episode reward 600.0, episode length 1059\n",
      "Time 14h 38m 12s, episode reward 650.0, episode length 1068\n",
      "Time 14h 39m 15s, episode reward 600.0, episode length 1122\n",
      "Time 14h 40m 17s, episode reward 600.0, episode length 895\n",
      "Time 14h 41m 21s, episode reward 545.0, episode length 1231\n",
      "Time 14h 42m 23s, episode reward 600.0, episode length 1062\n",
      "Time 14h 43m 26s, episode reward 460.0, episode length 888\n",
      "Time 14h 44m 29s, episode reward 775.0, episode length 1213\n",
      "Time 14h 45m 32s, episode reward 735.0, episode length 1214\n",
      "Time 14h 46m 35s, episode reward 545.0, episode length 1136\n",
      "Time 14h 47m 37s, episode reward 285.0, episode length 576\n",
      "Time 14h 48m 39s, episode reward 430.0, episode length 728\n",
      "Time 14h 49m 42s, episode reward 600.0, episode length 1178\n",
      "Time 14h 50m 45s, episode reward 570.0, episode length 1218\n",
      "Time 14h 51m 48s, episode reward 605.0, episode length 939\n",
      "Time 14h 52m 50s, episode reward 630.0, episode length 923\n",
      "Time 14h 53m 53s, episode reward 540.0, episode length 905\n",
      "Time 14h 54m 56s, episode reward 575.0, episode length 1104\n",
      "Time 14h 55m 59s, episode reward 515.0, episode length 1120\n",
      "Time 14h 57m 02s, episode reward 605.0, episode length 989\n",
      "Time 14h 58m 06s, episode reward 1130.0, episode length 1720\n",
      "Time 14h 59m 08s, episode reward 460.0, episode length 735\n",
      "Time 15h 00m 12s, episode reward 570.0, episode length 1356\n",
      "Time 15h 01m 15s, episode reward 545.0, episode length 977\n",
      "Time 15h 02m 19s, episode reward 490.0, episode length 970\n",
      "Time 15h 03m 23s, episode reward 735.0, episode length 1278\n",
      "Time 15h 04m 26s, episode reward 600.0, episode length 1133\n",
      "Time 15h 05m 29s, episode reward 545.0, episode length 967\n",
      "Time 15h 06m 32s, episode reward 440.0, episode length 863\n",
      "Time 15h 07m 35s, episode reward 600.0, episode length 1065\n",
      "Time 15h 08m 37s, episode reward 405.0, episode length 735\n",
      "Time 15h 09m 39s, episode reward 485.0, episode length 856\n",
      "Time 15h 10m 41s, episode reward 495.0, episode length 912\n",
      "Time 15h 11m 44s, episode reward 475.0, episode length 1017\n",
      "Time 15h 12m 47s, episode reward 545.0, episode length 1130\n",
      "Time 15h 13m 50s, episode reward 410.0, episode length 897\n",
      "Time 15h 14m 53s, episode reward 690.0, episode length 1336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 15h 15m 55s, episode reward 310.0, episode length 681\n",
      "Time 15h 16m 59s, episode reward 755.0, episode length 1307\n",
      "Time 15h 18m 03s, episode reward 680.0, episode length 1207\n",
      "Time 15h 19m 06s, episode reward 580.0, episode length 1133\n",
      "Time 15h 20m 08s, episode reward 485.0, episode length 763\n",
      "Time 15h 21m 11s, episode reward 600.0, episode length 1234\n",
      "Time 15h 22m 15s, episode reward 870.0, episode length 1680\n",
      "Time 15h 23m 18s, episode reward 580.0, episode length 1051\n",
      "Time 15h 24m 21s, episode reward 575.0, episode length 1071\n",
      "Time 15h 25m 23s, episode reward 305.0, episode length 634\n",
      "Time 15h 26m 25s, episode reward 435.0, episode length 814\n",
      "Time 15h 27m 27s, episode reward 540.0, episode length 1102\n",
      "Time 15h 28m 30s, episode reward 355.0, episode length 764\n",
      "Time 15h 29m 32s, episode reward 485.0, episode length 812\n",
      "Time 15h 30m 35s, episode reward 695.0, episode length 1136\n",
      "Time 15h 31m 38s, episode reward 570.0, episode length 1125\n",
      "Time 15h 32m 41s, episode reward 460.0, episode length 812\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    os.environ['OMP_NUM_THREADS'] = '1'\n",
    "    params = Params()\n",
    "    torch.manual_seed(params.seed)\n",
    "    env = create_atari_env(params.env_name)\n",
    "    shared_model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "    shared_model.share_memory()\n",
    "    optimizer = SharedAdam(shared_model.parameters(), lr=params.lr)\n",
    "    optimizer.share_memory()\n",
    "    \n",
    "    if params.task == 'train':\n",
    "        processes = []\n",
    "        p = mp.Process(target=test, args=(params.num_processes, params, shared_model))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "            \n",
    "        for rank in range(0, params.num_processes):\n",
    "            p = mp.Process(target=train, args=(rank, params, shared_model, optimizer))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "            \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "    elif params.task == 'eval':\n",
    "        shared_model.load_state_dict(torch.load(params.load_ckpt))\n",
    "        test(params.num_processes, params, shared_model)\n",
    "        \n",
    "    elif params.task == 'develop':\n",
    "        train(0, params, shared_model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
